{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLLqRxCN69XH",
        "outputId": "4c5215a4-c4f7-45f6-b570-c0df047a9dbf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 33.2M  100 33.2M    0     0  32.4M      0  0:00:01  0:00:01 --:--:-- 32.4M\n"
          ]
        }
      ],
      "source": [
        "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/agnews_clean.csv -O"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "\n",
        "spark = (SparkSession.builder\n",
        "         .master(\"local[*]\")\n",
        "         .appName(\"AG news\")\n",
        "         .getOrCreate()\n",
        "        )\n",
        "\n",
        "agnews = spark.read.csv(\"agnews_clean.csv\", inferSchema=True, header=True)\n",
        "\n",
        "# turning the second column from a string to an array\n",
        "import pyspark.sql.functions as F\n",
        "from pyspark.sql.types import ArrayType, StringType\n",
        "agnews = agnews.withColumn('filtered', F.from_json('filtered', ArrayType(StringType())))"
      ],
      "metadata": {
        "id": "MlOIgKZI7C8U"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# each row contains the document id and a list of filtered words\n",
        "agnews.show(5, truncate=30)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eBzxP2mc7RIJ",
        "outputId": "9e228d76-546f-4a77-9955-06c82173a4d5"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+------------------------------+\n",
            "|_c0|                      filtered|\n",
            "+---+------------------------------+\n",
            "|  0|[wall, st, bears, claw, bac...|\n",
            "|  1|[carlyle, looks, toward, co...|\n",
            "|  2|[oil, economy, cloud, stock...|\n",
            "|  3|[iraq, halts, oil, exports,...|\n",
            "|  4|[oil, prices, soar, time, r...|\n",
            "+---+------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "agnews.show(5, truncate=False)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PfMyVtICkj4m",
        "outputId": "285da685-9cf6-4812-b277-943e25a87359"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|_c0|filtered                                                                                                                                                                                                                                          |\n",
            "+---+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|0  |[wall, st, bears, claw, back, black, reuters, reuters, short, sellers, wall, street, dwindling, band, ultra, cynics, seeing, green]                                                                                                               |\n",
            "|1  |[carlyle, looks, toward, commercial, aerospace, reuters, reuters, private, investment, firm, carlyle, group, reputation, making, well, timed, occasionally, controversial, plays, defense, industry, quietly, placed, bets, another, part, market]|\n",
            "|2  |[oil, economy, cloud, stocks, outlook, reuters, reuters, soaring, crude, prices, plus, worries, economy, outlook, earnings, expected, hang, stock, market, next, week, depth, summer, doldrums]                                                   |\n",
            "|3  |[iraq, halts, oil, exports, main, southern, pipeline, reuters, reuters, authorities, halted, oil, export, flows, main, pipeline, southern, iraq, intelligence, showed, rebel, militia, strike, infrastructure, oil, official, said, saturday]     |\n",
            "|4  |[oil, prices, soar, time, record, posing, new, menace, us, economy, afp, afp, tearaway, world, oil, prices, toppling, records, straining, wallets, present, new, economic, menace, barely, three, months, us, presidential, elections]            |\n",
            "+---+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **PART 1: TF-IDF**"
      ],
      "metadata": {
        "id": "xomtuyeGZhT4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For TF**"
      ],
      "metadata": {
        "id": "1LFFntBccCUi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#number of terms t in d\n",
        "#map\n",
        "def map_word_in_docs(doc_id, words):\n",
        "    \"\"\" maps word to doc and associates 1 to count later\"\"\"\n",
        "    return [((word, doc_id), 1) for word in words]\n"
      ],
      "metadata": {
        "id": "zrROs--lZkeN"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def addab(a,b):\n",
        "  \"\"\" function to sum two counts\"\"\"\n",
        "  return a + b"
      ],
      "metadata": {
        "id": "X6MFa37mjgdg"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for each d counts of words\n",
        "#map\n",
        "def map_doc_lengths(word_in_docs):\n",
        "    \"\"\"\n",
        "    takes word t and number of times it appears in doc d and maps to doc d\n",
        "    (returns doc_id and count for each word)\n",
        "    \"\"\"\n",
        "    word, doc_id = word_in_docs[0]\n",
        "    count = word_in_docs[1]\n",
        "    return (doc_id, count)\n",
        "\n",
        "#reduce\n",
        "def reduce_doc_lengths(doc_id, counts):\n",
        "    \"\"\"\n",
        "    takes mapped doc_id and counts and returns sum of counts for each doc_id for total word counts in each doc d\n",
        "    \"\"\"\n",
        "    return (doc_id, sum(counts))"
      ],
      "metadata": {
        "id": "mPyTkML_ajRK"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#tf calculation\n",
        "def compute_tf(word_doc_count, doc_length_dict):\n",
        "    \"\"\"\n",
        "    for each word and doc_id, returns tf value (count/total_words in doc).\n",
        "    First splits word_doc_count into tuple and count for each word and doc_id\n",
        "    \"\"\"\n",
        "    (word, doc_id), count = word_doc_count\n",
        "    total_words = doc_length_dict[doc_id]\n",
        "    return ((word, doc_id), count / total_words)"
      ],
      "metadata": {
        "id": "AyaEWA5sbl2t"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For IDF**"
      ],
      "metadata": {
        "id": "sAWxiU9_cE-E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#number of docs containing word t\n",
        "\n",
        "def map_docs_in_word(doc_id, words):\n",
        "    \"\"\"\n",
        "    maps each word t in d to doc d (as a list, so for each t list is of docs that have that word)\n",
        "    \"\"\"\n",
        "    return list(set((word, doc_id) for word in words))\n"
      ],
      "metadata": {
        "id": "Of0XX3b5cG4c"
      },
      "execution_count": 41,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#IDF calculation\n",
        "def compute_idf(word, df, num_docs):\n",
        "    \"\"\"\n",
        "    for each word, returns idf value (log(# of d in D / #docs containing t))\n",
        "    \"\"\"\n",
        "    import math\n",
        "    return (word, math.log(num_docs / df))"
      ],
      "metadata": {
        "id": "78YA3d-udbl4"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**For TD-IDF Calculation**"
      ],
      "metadata": {
        "id": "CQ17gLFxeHQC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#TD-IDF Calculation\n",
        "def compute_tfidf(word_doc_id, tf, idf_dict):\n",
        "    \"\"\"\n",
        "    for each word and doc_id, returns tf-idf value (tf * idf)\n",
        "    \"\"\"\n",
        "    word, doc_id = word_doc_id\n",
        "    return ((word, doc_id), tf * idf_dict[word])\n"
      ],
      "metadata": {
        "id": "-ekOGyt_eAqc"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**AGNews TD-IDF Measure**"
      ],
      "metadata": {
        "id": "hGTJXopPmaeb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## tf ##\n",
        "agnews_rdd = agnews.select(\"_c0\", \"filtered\").rdd\n",
        "\n",
        "# dataset flattened (mapped) into word and doc pairs\n",
        "word_doc_pairs = agnews_rdd.flatMap(lambda row: map_word_in_docs(row[0], row[1]))\n",
        "#sum counts\n",
        "word_counts = word_doc_pairs.reduceByKey(addab)\n",
        "#map each doc to number of word entries in doc\n",
        "doc_word_counts = word_counts.map(map_doc_lengths)\n",
        "# sum for total words in doc\n",
        "doc_lengths = doc_word_counts.reduceByKey(addab)\n",
        "#make dictionary associating doc to word count\n",
        "doc_length_dict = dict(doc_lengths.collect())\n",
        "#computes tf for each doc and word pair\n",
        "tf_rdd = word_counts.map(lambda x: compute_tf(x, doc_length_dict))\n",
        "\n",
        "##idf##\n",
        "#maps dataset into word and doc pairs\n",
        "docs_with_word = agnews_rdd.flatMap(lambda row: map_docs_in_word(row[0], row[1]))\n",
        "#adds frequency of docs that contain each word\n",
        "word_doc_freq = docs_with_word.map(lambda x: (x[0], 1)).reduceByKey(addab)\n",
        "#counts total number of docs in dataset\n",
        "num_docs = agnews.count()\n",
        "#calculates idf term\n",
        "idf_rdd = word_doc_freq.map(lambda x: compute_idf(x[0], x[1], num_docs))\n",
        "#puts idf value in dictionary with each word and associated idf number\n",
        "idf_dict = dict(idf_rdd.collect())\n",
        "\n",
        "##tfidf calculation##\n",
        "tfidf_rdd = tf_rdd.map(lambda x: compute_tfidf(x[0], x[1], idf_dict))\n",
        "\n",
        "\n",
        "#to DF\n",
        "from pyspark.sql import Row\n",
        "#converts return of rdd into dataframe\n",
        "tfidf_rows = tfidf_rdd.map(lambda x: Row(d=x[0][1], tfidf_vector=[(x[0][0], x[1])]))\n",
        "#groups by each doc ID\n",
        "tfidf_grouped = tfidf_rows.reduceByKey(addab)\n",
        "\n",
        "tfidf_df = tfidf_grouped.map(lambda x: Row(d=x[0], tfidf_vector=x[1])).toDF()\n",
        "result_df = agnews.withColumnRenamed(\"_c0\", \"d\").join(tfidf_df, on=\"d\", how=\"left\")\n",
        "\n",
        "#returns first 5 doc id's and each word tf-idf pair per doc\n",
        "result_df.select(\"d\", \"tfidf_vector\").orderBy(\"d\").show(5, truncate=False)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QH4zrVe1fntA",
        "outputId": "32c27eb2-f265-4da2-9484-75614ee86f79"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "+---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|d  |tfidf_vector                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                     |\n",
            "+---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "|0  |[{claw, 0.499114829314058}, {back, 0.1892216338539946}, {black, 0.2953171727366614}, {reuters, 0.24754017186645658}, {short, 0.2773120373951269}, {band, 0.3643421454792778}, {ultra, 0.4125512394225831}, {green, 0.2877107940095433}, {wall, 0.5115985326511431}, {st, 0.2584728642725166}, {bears, 0.3372044607529448}, {sellers, 0.4468379768438066}, {street, 0.24678348986493034}, {dwindling, 0.4572386180709258}, {cynics, 0.563734318747707}, {seeing, 0.37743394553516213}]                                                                                                                                                                                                                                                                                                                            |\n",
            "|1  |[{looks, 0.1973537176743789}, {commercial, 0.2057832028092643}, {private, 0.1929050573011279}, {investment, 0.1890771769001148}, {firm, 0.15969712503706046}, {reputation, 0.2578098186776328}, {plays, 0.22418048797172685}, {quietly, 0.25188254045524316}, {bets, 0.27861293130724324}, {market, 0.13394932212703356}, {carlyle, 0.7168306746824437}, {toward, 0.1898997183872362}, {aerospace, 0.2581171817448437}, {reuters, 0.1650267812443044}, {group, 0.12468100563149095}, {making, 0.1698717076460444}, {well, 0.17053284421704767}, {timed, 0.324478643568105}, {occasionally, 0.33274321954270536}, {controversial, 0.20949395177306526}, {defense, 0.1751279339938823}, {industry, 0.15043731768548949}, {placed, 0.2284965552404658}, {another, 0.14507889141437585}, {part, 0.16022031730914288}]|\n",
            "|2  |[{oil, 0.13908157105107033}, {cloud, 0.295159450642955}, {stocks, 0.14976769101715193}, {outlook, 0.4265073217271922}, {reuters, 0.18565512889984243}, {plus, 0.24449073714833106}, {expected, 0.16094627131903613}, {hang, 0.30475018305843793}, {week, 0.13121900794126834}, {summer, 0.22694739048609625}, {doldrums, 0.3770252270329423}, {economy, 0.3721400726458204}, {soaring, 0.2596334462817101}, {crude, 0.197241148492091}, {prices, 0.14472559202114177}, {worries, 0.23009353850726894}, {earnings, 0.1792714404894228}, {stock, 0.17879168082328206}, {market, 0.15069298739291276}, {next, 0.14062721303262238}, {depth, 0.31343954772064864}]                                                                                                                                                   |\n",
            "|3  |[{iraq, 0.23809526243476142}, {halts, 0.27365396741681164}, {southern, 0.336553609483104}, {pipeline, 0.4720829409342409}, {authorities, 0.18159366801541998}, {export, 0.23862435123782139}, {intelligence, 0.20782569445751425}, {infrastructure, 0.22959926718225876}, {official, 0.15149485319300557}, {said, 0.06593367258642661}, {saturday, 0.12197305137253434}, {oil, 0.35763832555989516}, {exports, 0.2146590164054526}, {main, 0.36492623402353547}, {reuters, 0.15913296762843637}, {halted, 0.2557691357056513}, {flows, 0.2774168429760197}, {showed, 0.1743365558077232}, {rebel, 0.18209445014364567}, {militia, 0.2252006141545402}, {strike, 0.17411586950893898}]                                                                                                                            |\n",
            "|4  |[{oil, 0.22253051368171256}, {soar, 0.2306791247647116}, {new, 0.1271397626254836}, {menace, 0.5747440955975784}, {us, 0.1669859687392097}, {world, 0.09332201126546583}, {toppling, 0.27964532733021175}, {straining, 0.2904044404056468}, {present, 0.22209684830286883}, {barely, 0.21935019724396657}, {three, 0.10314988960754677}, {months, 0.14002501854271598}, {presidential, 0.1480257381794347}, {prices, 0.23156094723382684}, {time, 0.10623532598945136}, {record, 0.1232987151692413}, {posing, 0.2589223867776184}, {economy, 0.14885602905832815}, {afp, 0.2559170042376607}, {tearaway, 0.3918885216630942}, {records, 0.19759033440942064}, {wallets, 0.2665151844733088}, {economic, 0.14782686453681568}, {elections, 0.16009904796740967}]                                                 |\n",
            "+---+-----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
            "only showing top 5 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Part 2: SVM Objective Function**"
      ],
      "metadata": {
        "id": "r4h2LN9tP4GM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def map_summation_term(row, w, b):\n",
        "    \"\"\"\n",
        "    summation component of the hinge loss function (before it is summed, terms are mapped together)\n",
        "    \"\"\"\n",
        "    from numpy import dot\n",
        "    x_i, y_i = row\n",
        "    margin = y_i * (dot(w, x_i) + b)\n",
        "    return max(0.0, 1.0 - margin)\n",
        "\n",
        "#function addab still works for summing over all i = 1 to n, so no additional summation function needed"
      ],
      "metadata": {
        "id": "xS5iY4oFRqMl"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def loss_SVM(w, b, X, y):\n",
        "    \"\"\"\n",
        "    SVM objective function\n",
        "    \"\"\"\n",
        "    lambd = 1\n",
        "    #combines X and Y into pairs in rdd\n",
        "    Xy_rdd = X.zip(y)\n",
        "\n",
        "    n = Xy_rdd.count()\n",
        "    #this isn't the summation itself, it is the mapping of each term in that component before it is summed\n",
        "    summation = Xy_rdd.map(lambda row: map_summation_term(row, w, b))\n",
        "    #here is where each term is added together\n",
        "    total_summation = summation.reduce(addab)\n",
        "    #lambda and norm of w term combined\n",
        "    from numpy.linalg import norm\n",
        "    reg_term = lambd * (norm(w) ** 2)\n",
        "\n",
        "    return reg_term + (1 / n) * total_summation\n"
      ],
      "metadata": {
        "id": "nXrAD1zdQ9yZ"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/w.csv -O\n",
        "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/bias.csv -O\n",
        "!curl https://raw.githubusercontent.com/mosesyhc/de300-2025sp-class/refs/heads/main/data_for_svm.csv -O"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zbwqKu4rQMB0",
        "outputId": "479784a7-7092-43fc-bde4-86ddb9776020"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100  1391  100  1391    0     0   9567      0 --:--:-- --:--:-- --:--:--  9593\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100    22  100    22    0     0    149      0 --:--:-- --:--:-- --:--:--   148\n",
            "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
            "                                 Dload  Upload   Total   Spent    Left  Speed\n",
            "100 61.9M  100 61.9M    0     0  81.2M      0 --:--:-- --:--:-- --:--:-- 81.3M\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"SVM\").getOrCreate()\n",
        "\n",
        "df = spark.read.csv(\"data_for_svm.csv\", header=False, inferSchema=True)"
      ],
      "metadata": {
        "id": "po3G-AFYRMIF"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "X = df.rdd.map(lambda row: [float(row[i]) for i in range(64)])\n",
        "y = df.rdd.map(lambda row: int(row[64]))\n",
        "w = np.array(pd.read_csv(\"w.csv\", header=None)).flatten()\n",
        "b = float(pd.read_csv(\"bias.csv\", header=None).iloc[0, 0])\n",
        "\n",
        "loss = loss_SVM(w, b, X, y)\n",
        "print(\"SVM Objective Loss:\", loss)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cIse9GzTRYvI",
        "outputId": "db6e236e-38b6-46fc-c145-6e27ba905251"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SVM Objective Loss: 1.0029686410368055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#SVM Classifier\n",
        "\n",
        "def map_classifier(row, w, b):\n",
        "  \"\"\" makes estimate of y by taking dot product of w and xi and returns 1 if it is positive and -1 if negative\"\"\"\n",
        "  from numpy import dot\n",
        "  z = (dot(w,row) + b)\n",
        "  if z >= 0:\n",
        "    return 1\n",
        "  else:\n",
        "    return -1\n"
      ],
      "metadata": {
        "id": "H8w474RFWP2K"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "predictions_rdd = X.map(lambda x: map_classifier(x, w, b))\n",
        "predictions_rdd.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u7oFde0CXMMh",
        "outputId": "5ca01b35-d195-40c0-9c5c-ad1afc16e4b5"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[-1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " 1,\n",
              " -1,\n",
              " 1,\n",
              " -1,\n",
              " -1,\n",
              " 1,\n",
              " 1,\n",
              " ...]"
            ]
          },
          "metadata": {},
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pred_true_rdd = X.zip(y).map(lambda row: (map_classifier(row[0], w, b), row[1]))\n",
        "\n",
        "# Count correct number of predictions\n",
        "correct = pred_true_rdd.filter(lambda pair: pair[0] == pair[1]).count()\n",
        "total = pred_true_rdd.count()\n",
        "\n",
        "#accuracy is correct number of predictions divided by the number of predictions made (entries in dataset)\n",
        "accuracy = correct / total\n",
        "print(\"Accuracy:\", accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P8GtL5u0xMPM",
        "outputId": "0b38025a-dd71-4a11-8a2d-1d16faa97d2c"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.5018\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Generative AI Disclosure"
      ],
      "metadata": {
        "id": "1swT9abUwQEZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In constructing rdds for the tf-idf function, ChatGPT was used to understand the .flatMap in implementing map_docs_in_word because it returns many outputs for one input which was needed as one word appears in many docs (and one doc contains many words), so it allows for this input. I asked if there was a way to associate multiple entries to the same key term. Additionally, I asked about a way for an rdd to count all the entries with the same term to group and sum them, to which it provided .reduceByKey() to count frequencies, used again for the td-idf function. The python equivalent would be .groupBy().sum()."
      ],
      "metadata": {
        "id": "HtBxWGA8wT4Z"
      }
    }
  ]
}